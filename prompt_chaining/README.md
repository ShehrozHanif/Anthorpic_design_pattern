# 🚀 Deep Dive into Our Prompt Chaining Implementation with Anthropic AI

## 📌 What is Prompt Chaining?
Prompt Chaining is a powerful design pattern where multiple prompts are connected in a logical sequence. 
Instead of relying on a single LLM call, we break the process into multiple steps, ensuring:

 ✅ **Better accuracy** – Each step refines the data before passing it forward.
 
 ✅ **Input validation** – If an input fails at any stage, the process stops immediately instead of propagating bad data.
 
 ✅ **Traceability** – Every step is saved and logged, making debugging easier.

 ## 🛠️ Code Breakdown: Step-by-Step
 
 ### 1️⃣ Importing Required Modules
 Our code starts by importing essential libraries:

            from crewai.flow.flow import Flow, start, listen  
            from litellm import completion  
            import os  

✅ **Flow**, **start**, and **listen** from crewai.flow.flow help in orchestrating the multi-step process.

✅ **completion** from litellm is used to call and process responses from LLMs.

✅ **os** handles file operations, such as saving outputs.

### 2️⃣ Defining Classes and Decorators for Prompt Chaining
We use Python classes and decorators to modularize the AI workflow:

#### 🔹 Step 1: Input Validation (LLM1)
* First, we validate the input to ensure it meets the predefined requirements.

* If validation fails, the process stops here.
